{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOJ9npnFEi-d"
   },
   "source": [
    "# Applied Data Science and Machine Intelligence\n",
    "## A program by IITM and TalentSprint\n",
    "### Practice Notebook: Data Wrangling (Ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBkjMtSgFBuv"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpfeKNyLFEbX"
   },
   "source": [
    "At the end of the experiment, you will be able to :\n",
    "\n",
    "* load the dataframe using pandas\n",
    "* perform various operations in the dataframe like:\n",
    "     * renaming the columns,\n",
    "     * indexing and data retrieval (both column wise and row wise),\n",
    "     * drop and add a columns in the dataframe,\n",
    "     * integration of datasets,\n",
    "     * data cleaning,  \n",
    "     * data tansformation operations like standardization, label encoding, feature scaling, etc,\n",
    "     * find the correlation among different features of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km0a903iYVhN"
   },
   "source": [
    "#### Exercise 1: Loading the data using Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0bLbYlSXrsJ"
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "# numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EWdwD6kN_Hz"
   },
   "source": [
    "Dataset is chosen from the [UCI repository](http://archive.ics.uci.edu/ml/datasets/Forest+Fires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u20eO4_omaO8"
   },
   "source": [
    "#### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp3Mc8_AmkB6"
   },
   "source": [
    "#### **Forest Fires Dataset**\n",
    "1. Number of Instances: 517\n",
    "2. Number of Attributes: 13\n",
    "3. Attribute Information:\n",
    "    \n",
    "      i. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "\n",
    "      ii. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "\n",
    "      iii. month - month of the year: 'jan' to 'dec'\n",
    "\n",
    "      iv. day - day of the week: 'mon' to 'sun'\n",
    "      \n",
    "      v. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "\n",
    "      vi. DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "      \n",
    "      vii. DC - DC index from the FWI system: 7.9 to 860.6\n",
    "\n",
    "      viii. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "     \n",
    "     ix. te mp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "      \n",
    "      x. RH - relative humidity in %: 15.0 to 100\n",
    "      \n",
    "      xi. wind - wind speed in km/h: 0.40 to 9.40\n",
    "      \n",
    "      xii. rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "      \n",
    "      xiii. area - the burned area of the forest (in ha): 0.00 to 1090.84 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNNcTjrZ6yHX"
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "!wget -qq https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTn6eVQSK5i5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('forestfires.csv')\n",
    "# display first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef8UE3eUHNEn"
   },
   "source": [
    "In the above dataframe, first row contains the duplicate headers, skip the header using `skiprows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX01564sHKkD"
   },
   "outputs": [],
   "source": [
    "# using pandas read the csv file\n",
    "df = pd.read_csv('forestfires.csv',skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TffBhDVYd0W"
   },
   "source": [
    "#### Exercise 2: Rename the columns\n",
    "\n",
    "Based on the dataset attributes, we will rename RH column to Relative Humidity and other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8iqwrMlYhPL"
   },
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "df = df.rename(columns = {'RH':'Relative Humidity', 'X':'X-axis', 'Y':'Y-axis'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsv9c537Yhmm"
   },
   "source": [
    "#### Exercise 3: Indexing and data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PuhaPpsp_R0"
   },
   "outputs": [],
   "source": [
    "# setting temp as index column\n",
    "df.set_index(\"temp\", inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc7Uh0_8sAHW"
   },
   "source": [
    "Earlier the index is temp column, it can be removed by resetting the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RM0INV0usmB"
   },
   "outputs": [],
   "source": [
    "# resetting index\n",
    "df.reset_index(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU6L1HnPQigQ"
   },
   "source": [
    "Retrieve column-wise  data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH9OYk1gYnZs"
   },
   "outputs": [],
   "source": [
    "# retrieving columns\n",
    "X1 = df[\"wind\"]\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smcAA0c2gaES"
   },
   "outputs": [],
   "source": [
    "# retrieving multiple columns\n",
    "X2 = df[[\"wind\", \"rain\", \"area\"]]\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5hVVNqwQ5Ix"
   },
   "source": [
    "`loc` is label-based, which means that you have to specify rows and columns based on their row and column labels.\n",
    "\n",
    "syntax: ` loc[row_label, column_label]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyZjW5G8hM8r"
   },
   "outputs": [],
   "source": [
    "# retrieving rows by loc method\n",
    "rows_1 = df.loc[[4,5,6,7,8,9,10],'temp']\n",
    "print(rows_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO6mIqINQ9FT"
   },
   "source": [
    "`iloc` is integer position-based, so you have to specify rows and columns by their integer position values (0-based integer position).\n",
    "\n",
    "syntax: `iloc[row_position, column_position]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MRLXqV-lWzC"
   },
   "outputs": [],
   "source": [
    "# retrieving data using integer location\n",
    "sub_data = df.iloc[0:10,:3]\n",
    "sub_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE1uJQ48SfaZ"
   },
   "source": [
    "Index of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WECpwJho8Iv"
   },
   "outputs": [],
   "source": [
    "# start the index from 1\n",
    "df.index = range(1,len(df)+1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K13otw6dYp7Y"
   },
   "source": [
    "#### Exercise 4: Drop the columns\n",
    "\n",
    "* Identify the irrelevant columns and drop\n",
    "\n",
    "day and month can be added into one single column by dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPzBJsgQLfXx"
   },
   "outputs": [],
   "source": [
    "# new column date\n",
    "df['date'] = df['day']+\", \"+df['month']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLqELAX6YrZU"
   },
   "outputs": [],
   "source": [
    "# drop columns \n",
    "df.drop([\"day\",'month'], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV7bCC9ZYs6h"
   },
   "source": [
    "#### Exercise 5: Data Integration\n",
    "\n",
    "Create the two dataframes representing different information and combine them as one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBJxze4QjQh9"
   },
   "outputs": [],
   "source": [
    "# Students dataframe\n",
    "df1 = pd.DataFrame({\"Name\": [\"Aman\", \"Joy\", \"Vinay\", \"Jack\", \"Rita\", \"Robin\", \"Sam\"], \n",
    "                    \"Rollno\": [1, 2, 3, 4, 5, 6, 7]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kbf7jlyNaGe"
   },
   "outputs": [],
   "source": [
    "# Marks dataframe\n",
    "df2 = pd.DataFrame({\"Rollno\": [1, 2, 3, 4, 5, 6, 7],\n",
    "                    \"Maths\": [40, 50, 30, 60, 82, 74, 25],\n",
    "                    \"English\": [90, 84, 48, 64, 45, np.nan, 46],\n",
    "                    \"Science\": [66, 54, 20, np.nan, 90, 48, 28]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbqwZNUoNfDd"
   },
   "source": [
    "Merge both the dataframes side by side, as they are representing different information with common identifier `Rollno`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcuzAQZ_f9fX"
   },
   "outputs": [],
   "source": [
    "# Merge df1 and df2 on the 'Rollno' and 'Roll No' columns\n",
    "df_merge = pd.merge(df1, df2, on=['Rollno'])\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aqa4NpYAizPQ"
   },
   "source": [
    "To know more about other merge operations, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnjAWesEjoyH"
   },
   "outputs": [],
   "source": [
    "# Combine two dataframes along columns\n",
    "df_concat = pd.concat([df1, df2],axis=1)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diovpnLclRSf"
   },
   "source": [
    "To know more about other concat operations, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsjM8O6oYxkR"
   },
   "source": [
    "#### Exercise 6: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVd29sLnmmUh"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df_merge.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSauUUl6mW7_"
   },
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "df_merge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tYrZ44Nnsh0"
   },
   "outputs": [],
   "source": [
    "# Show the mean of each column\n",
    "df_merge.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8sflewSmdas"
   },
   "outputs": [],
   "source": [
    "# Filling missing values with the mean value of that column\n",
    "df_filled = df_merge.fillna(df_merge.mean())\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz7DokQLYz3r"
   },
   "source": [
    "#### Exercise 7: Data Transformation\n",
    "\n",
    "Create a Total marks column and identify the Result based on the marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbcaBppUp6Za"
   },
   "outputs": [],
   "source": [
    "# Add 'Total' marks column \n",
    "df_filled[\"Total\"] = df_filled.iloc[:, -3:].sum(axis=1)\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CqJFRzTt1nn"
   },
   "outputs": [],
   "source": [
    "# Add 'Result' column\n",
    "for i in range(len(df_filled)):\n",
    "    # consider above 35% as pass out of total 300 marks\n",
    "    if df_filled.loc[i, \"Total\"] > (0.35 * 300):\n",
    "        df_filled.loc[i, \"Result\"] = \"Pass\"\n",
    "    else:\n",
    "        df_filled.loc[i, \"Result\"] = \"Fail\"\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY3vW6x4Ugjj"
   },
   "source": [
    "**LabelEncoder:** Sklearn provides a very efficient tool for encoding the levels of categorical features into numeric values. LabelEncoder encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCVFvz-KY2IE"
   },
   "outputs": [],
   "source": [
    "# Label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_filled[\"Result\"])\n",
    "label = le.transform(df_filled[\"Result\"])\n",
    "df_filled[\"Result label\"] = label\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaVxskq-u9oL"
   },
   "source": [
    "To know more about LabelEncoder, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pZSOMrWU1Ob"
   },
   "source": [
    "**Feature Scaling:** It is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.\n",
    "\n",
    "To know more about StandardScaler, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTUWb9M2dwHg"
   },
   "outputs": [],
   "source": [
    "# StandardScaler from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_scaled_sc = df_filled.copy()\n",
    "\n",
    "#defining the StandardScaler variable\n",
    "sc = StandardScaler()\n",
    "sc.fit(df_scaled_sc[[\"Maths\", \"English\", \"Science\"]])\n",
    "\n",
    "# using .transform to apply StandardScaler operation \n",
    "scaled = sc.transform(df_scaled_sc[[\"Maths\", \"English\", \"Science\"]])\n",
    "df_scaled_sc[[\"Maths_scaled_StandardScaler\", \"English_scaled_StandardScaler\", \"Science_scaled_StandardScaler\"]] = scaled\n",
    "df_scaled_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQnSRWgVWbp9"
   },
   "source": [
    "To know more about MinMaxScaler, check this [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47NS16bCwx2l"
   },
   "outputs": [],
   "source": [
    "# MinMaxScaler from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_scaled_mms = df_filled.copy()\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "# transform features by scaling each feature to a given range e.g. [0,1]\n",
    "mms.fit(df_scaled_mms[[\"Maths\", \"English\", \"Science\"]])\n",
    "\n",
    "scaled = mms.transform(df_scaled_mms[[\"Maths\", \"English\", \"Science\"]])\n",
    "df_scaled_mms[[\"Maths_scaled_MinMaxScaler\", \"English_scaled_MinMaxScaler\", \"Science_scaled_MinMaxScaler\"]] = scaled\n",
    "df_scaled_mms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNWW8Sc7Y2u2"
   },
   "source": [
    "#### Exercise 8: Features correlation\n",
    "\n",
    "**Data Correlation:** Is a way to understand the relationship between multiple variables and attributes in your dataset. Using Correlation, you can get some insights such as:\n",
    "One or multiple attributes depend on another attribute or a cause for another attribute.\n",
    "One or multiple attributes are associated with other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvUCv28eZAGz"
   },
   "outputs": [],
   "source": [
    "# Correlation between features\n",
    "df_scaled_sc.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6OIDA4Q1ua7"
   },
   "outputs": [],
   "source": [
    "# Heatmap of correlation matrix\n",
    "import seaborn as sns\n",
    "sns.heatmap(df_scaled_sc.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVdkpywPxted"
   },
   "outputs": [],
   "source": [
    "# Change the marks to 35 if it is below 35 for all three subjects\n",
    "df = df_filled[[\"Maths\", \"English\", \"Science\"]]\n",
    "# Marks below 35\n",
    "df.mask(df < 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvJRBRWF0-TW"
   },
   "outputs": [],
   "source": [
    "# Assign marks\n",
    "df.mask(df < 35, 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzAIF4lV1YGo"
   },
   "source": [
    "To know more about df.mask() operation, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M0_Practice_04_Data_Wrangling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
